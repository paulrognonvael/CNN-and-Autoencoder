---
title: "Task 2 Statistical Learning 2020"
author: "Group 3"
date: '`r format(Sys.Date(),"%B %eth, %Y")`'
output:
  pdf_document:
    number_sections: yes
    toc: yes
    includes:
      in_header: my_header.tex
  # html_document:
  #   df_print: paged
  #   toc: yes
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, warning = FALSE, error = FALSE, message = FALSE,eval = FALSE,
  tidy.opts = list(width.cutoff = 55)
)
```


```{r,echo=TRUE,eval=TRUE}
# libraries
library(keras)
library(rstatix)
library(tidyverse)
library(caret)
library(ggplot2)
library(gridExtra)
library(tfruns)
library(kableExtra)
library(caret)
library(plyr)


# set seed for reproductibility
set.seed(42)
tensorflow::tf$random$set_seed(42)

#load saved data
load("data_q1to8.RData")
```

# Question 1&2 

***Normalize the images. Split the dataset into 500 training /100 validation /100 test. Try to balance the two classes.***

We create a function `random_files` to automatically split the data in the required folder structure and in training, validation and test sets.

```{r}
random_files <- function(readpath, writepath, type, train_size, val_size, test_size, pattern = "png$|PNG$"){
####################################################################
# readpath and writepath = path to folders with files to select and where to write them                     
#                                                                            
# train_size, val_size, test_size = percentage or number of recordings to select. If value is 
#   between 0 and 1 percentage of files is assumed, if value greater than 1, 
#   number of files is assumed                                               
#                                                                            
# pattern = file extension to select.       
# type = normal(1) or effusion(2) types   
####################################################################
      
      #### copy original data in temp folder for separation 
      temp_path = file.path(writepath, 'temp')
      dir.create(temp_path)
      file.copy(readpath, temp_path, recursive=TRUE)
      if (type == 1){
        temp_type_path = file.path(temp_path, 'normal')
      }else if(type == 2){
        temp_type_path = file.path(temp_path, 'effusion')
      }else {print("Incorrect type")}
      
      # Get file list with full path and file names
      files <- list.files(temp_type_path, full.names = TRUE, pattern = pattern)
      file_names <- list.files(temp_type_path, pattern = pattern)
      
      # Select the desired % or number of file by simple random sampling 
      randomize <- sample(seq(files))
      files2analyse <- files[randomize]
      names2analyse <- file_names[randomize]
      if(train_size <= 1){
        size <- floor(train_size * length(files))
      }else{
        size <- train_size
      }
      files2analyse <- files2analyse[(1:size)]
      names2analyse <- names2analyse[(1:size)]
    
      ##### training
    
      # Create folder to output
      if (type == 1){
        results_folder <- paste0(writepath, '/train/normal')
      }else if(type == 2){
        results_folder <- paste0(writepath, '/train/effusion')
      }else {print("Incorrect type")}
      dir.create(results_folder, recursive=TRUE)
    
      # copy files
      for(i in seq(files2analyse)){
        file.rename(from = files2analyse[i], to = paste0(results_folder, "/", names2analyse[i]) )
      }
    
      ##### validation
    
      files <- list.files(temp_type_path, full.names = TRUE, pattern = pattern)
      file_names <- list.files(temp_type_path, pattern = pattern)
      
      # Select the desired % or number of file by simple random sampling
      randomize <- sample(seq(files))
      files2analyse <- files[randomize]
      names2analyse <- file_names[randomize]
      if(val_size <= 1){
        size <- floor(val_size * length(files))
      }else{
        size <- val_size
      }
      files2analyse <- files2analyse[(1:size)]
      names2analyse <- names2analyse[(1:size)]
    
      if (type == 1){
        results_folder <- paste0(writepath, '/validation/normal')
      }else if(type == 2){
        results_folder <- paste0(writepath, '/validation/effusion')
      }else {print("Incorrect type")}
      dir.create(results_folder, recursive=TRUE)
    
      # copy files
      for(i in seq(files2analyse)){
        file.rename(from = files2analyse[i], to = paste0(results_folder, "/", names2analyse[i]) )
    
    
      }

      ##### test
      
      files <- list.files(temp_type_path, full.names = TRUE, pattern = pattern)
      file_names <- list.files(temp_type_path, pattern = pattern)
      
      # Select the desired % or number of file by simple random sampling
      randomize <- sample(seq(files))
      files2analyse <- files[randomize]
      names2analyse <- file_names[randomize]
      if(test_size <= 1){
        size <- floor(test_size * length(files))
      }else{
        size <- test_size
      }
      files2analyse <- files2analyse[(1:size)]
      names2analyse <- names2analyse[(1:size)]
      
      if (type == 1){
        results_folder <- paste0(writepath, '/test/normal')
      }else if(type == 2){
        results_folder <- paste0(writepath, '/test/effusion')
      }else {print("Incorrect type")}
      dir.create(results_folder, recursive=TRUE)
      
      
      # copy files
      for(i in seq(files2analyse)){
        file.rename(from = files2analyse[i], to = paste0(results_folder, "/", names2analyse[i]) )
      }
      
      ### remove the temp folder
      unlink(file.path(writepath, 'temp'),recursive = TRUE)
      
}
```

We then execute our function and define the training, validation and test folders.

```{r,echo=TRUE}
# execute function
random_files('rxtorax/normal','rxtorax', 1, 250, 50, 50, pattern = "png$|PNG$")
random_files('rxtorax/effusion','rxtorax', 2, 250, 50, 50, pattern = "png$|PNG$")
```


```{r,echo=TRUE}
# define training, validation and test folders
train_dir<-"rxtorax/train"
validation_dir<-"rxtorax/validation"
test_dir<-"rxtorax/test"
```

We then use `image_data_generator` to normalize our data and `flow_images_from_directory` to define batches, resize our images and keep a unique channel. We start with a batch size of 25.

```{r,echo=TRUE}
b_size <- 25

train_datagen <- image_data_generator(rescale = 1/255)
train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  color_mode = "grayscale",
  target_size = c(64, 64),
  batch_size = b_size,
  class_mode = "binary"
)

validation_datagen <- image_data_generator(rescale = 1/255)
validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  color_mode = "grayscale",
  target_size = c(64, 64),
  batch_size = b_size,
  class_mode = "binary"
)

test_datagen <- image_data_generator(rescale = 1/255)
test_generator <- flow_images_from_directory(
  test_dir,
  test_datagen,
  color_mode = "grayscale",
  target_size = c(64, 64),
  batch_size = b_size,
  class_mode = "binary",
  classes = c("effusion","normal"),
  shuffle = FALSE
)
# Now we have the images in the required format: 64x64 with a unique channel
```


# Question 3

***Implement a Convolutional Neural Network (CNN).***

We then define a Convolutional Neural Network, with 2 convolution layers, each with its own pooling layer. Then we add a flatten layer and a dropout layer. At the bottom of the network we have two fully connected layers with 128 and 32 nodes respectively. Finally, the output layer has one unit and a sigmoid activation function. The total number of trainable parameters is 816 673. 
 
```{r,eval=TRUE,echo=TRUE}
model <- keras_model_sequential() %>%
  # first convolutional hidden layer and max pooling
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                activation = "relu",input_shape = c(64, 64, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  # second convolutional hidden layer and max pooling
  layer_conv_2d(filters = 32, kernel_size = c(3, 3),
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate=0.4) %>%
  # Outputs from dense layer are projected onto output layer
  layer_dense(units = 128, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

We compile and fit the model with a binary crossentropy loss function, compute the accuracy and use 13 epochs. We save our model in the attached file `cnn_model_batch25.h5`.

```{r,echo=TRUE}
# Compile the model
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)

# Fit the model
history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 500/b_size,
  epochs = 13,
  validation_data = validation_generator,
  validation_steps = 100/b_size
)
model %>% save_model_hdf5("cnn_model_batch25.h5")
```

We obtain the following loss and accuracy values across epochs on the training and validation sets. We see the training and validation values stagnate and coincide after the 10th epoch. More than 13 epochs led to overfitting.

```{r,eval=TRUE,fig.cap="\\label{fig:fig1} CNN accuracy on training and validation set",out.extra = "", fig.pos = 'h!', fig.align="center"}
plot(history)
```

# Question 4

***Tune the hyperparameter batch_size checking the values 25, 35, 50.***

We use `tfruns` to tune the batch size hyperparameter.

```{r,echo=TRUE}
b_s <- c(25,35,50)
for (b in b_s) {
  training_run("cnn_flags.r", flags = c(batch_size = b))
}
tuning_res <- tfruns::ls_runs()
write_csv(tuning_res,"tfruns_res_batchsize.csv")
```

```{r}
tfruns_res_batchsize <- read_csv("tfruns_res_batchsize.csv")
best_batch <- tfruns_res_batchsize$flag_batch_size[which.max(tfruns_res_batchsize$metric_val_acc)]
```

In Table \ref{tab:tab1}, we present the output of the tuning. The loss and accuracy values are very close to each other, the differences are probably not significant. We find a batch size of `r best_batch` gives the best accuracy value on the validation set so we set our batch size at `r best_batch`, refit our model with that size and save our tuned model in the attached file `cnn_model_bestbatch.h5`.

```{r,eval=TRUE}
tfruns_res_batchsize %>% select(c(6,2,3,4,5)) %>% kable(
  digits = 3,
  col.names = c("Batch size", "Train. loss", "Val. loss","Train. acc.","Val. acc"),
  align = "c",
  caption = "\\label{tab:tab1} Batch size tuning results - loss and accuracy"
)
```

```{r,echo =TRUE}
set.seed(43)
tensorflow::tf$random$set_seed(43)

b_size <- best_batch

# initialise the model
model_bestbatch <- keras_model_sequential() %>%
  # first convolutional hidden layer and max pooling
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                activation = "relu",input_shape = c(64, 64, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  # second convolutional hidden layer and max pooling
  layer_conv_2d(filters = 32, kernel_size = c(3, 3),
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate=0.4) %>%
  # Outputs from dense layer are projected onto output layer
  layer_dense(units = 128, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

# Compile the model
model_bestbatch %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)

# Fit the model
history_bestbatch <- model_bestbatch %>% fit_generator(
  train_generator,
  steps_per_epoch = 500/b_size,
  epochs = 13,
  validation_data = validation_generator,
  validation_steps = 100/b_size
)
model_bestbatch %>% save_model_hdf5("cnn_model_bestbatch.h5")
```


# Question 5

***Assess the performance of the CNN predicting the categories of test images and obtain the confusion matrix.***

We use `predict` to predict the class of our test images and gather the results in a dataframe.

```{r,echo =TRUE}
predict <- model_bestbatch %>% predict_generator(
  test_generator,
  steps = 100/b_size)

stat_df <- tibble(predict[1:100,], test_generator$filenames, test_generator$classes)
  # assign prediction probability for filenames
colnames(stat_df) <- c(
    "predict_proba",
    "filename",
    "class") 
stat_df <- stat_df %>%
  mutate(predict_proba = as.double(predict_proba)) %>%
           mutate(predicted_class = ifelse(predict_proba > 0.5, 1, 0)) %>%
           mutate(predicted_class = as.integer(predicted_class)) %>% 
  mutate(label_name = ifelse(predicted_class == 0, "effusion", "normal"))

test_accuracy <- mean(stat_df$class==stat_df$predicted_class)

```

We obtain an accuracy of 0.78 on the test set. The confusion matrix is:

```{r,eval =TRUE}
confusionMatrix(as.factor(stat_df$predicted_class),as.factor(stat_df$class))
```


# Question 6 & 7

***Re-fit the CNN including data augmentation. Was the use of augmentation an improvement? Compare these two CNN models.***

We use `image_data_generator` to augment the training set of images and fit our model again with all the hyperparameters equal to our model without augmentation and the same seed. We save our augmented model in the attached file `cnn_model_bestbatch_augmented.h5`.

```{r,echo =TRUE}
datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE
)

set.seed(43)
tensorflow::tf$random$set_seed(43)

b_size <- best_batch

train_generator <- flow_images_from_directory(
  train_dir,
  datagen,
  color_mode = "grayscale",
  target_size = c(64, 64),
  batch_size = b_size,
  class_mode = "binary"
)

validation_datagen <- image_data_generator(rescale = 1/255)
validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  color_mode = "grayscale",
  target_size = c(64, 64),
  batch_size = b_size,
  class_mode = "binary"
)

test_datagen <- image_data_generator(rescale = 1/255)
test_generator <- flow_images_from_directory(
  test_dir,
  test_datagen,
  color_mode = "grayscale",
  target_size = c(64, 64),
  batch_size = b_size,
  class_mode = "binary",
  classes = c("effusion","normal"),
  shuffle = FALSE
)

# initialise the model
model_bestbatch_aug <- keras_model_sequential() %>%
  # first convolutional hidden layer and max pooling
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                activation = "relu",input_shape = c(64, 64, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  # second convolutional hidden layer and max pooling
  layer_conv_2d(filters = 32, kernel_size = c(3, 3),
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate=0.4) %>%
  # Outputs from dense layer are projected onto output layer
  layer_dense(units = 128, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


# Compile the model
model_bestbatch_aug %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)

# Fit the model
history2 <- model_bestbatch_aug %>% fit_generator(
  train_generator,
  steps_per_epoch = 500/b_size,
  epochs = 13,
  validation_data = validation_generator,
  validation_steps = 100/b_size
)
model_bestbatch_aug %>% save_model_hdf5("cnn_model_bestbatch_augmented.h5")

# Prediction
predict_bestbatch_aug <- model_bestbatch_aug %>% predict_generator(
  test_generator,
  steps = 100/b_size)

stat_df_bestbatch_aug <- tibble(predict_bestbatch_aug, test_generator$filenames, test_generator$classes)
  # assign prediction probability for filenames
colnames(stat_df_bestbatch_aug) <- c(
    "predict_proba",
    "filename",
    "class") 
stat_df_bestbatch_aug <- stat_df_bestbatch_aug %>%
  mutate(predict_proba = as.double(predict_proba)) %>%
           mutate(predicted_class = ifelse(predict_proba > 0.5, 1, 0)) %>%
           mutate(predicted_class = as.integer(predicted_class)) %>% 
  mutate(label_name = ifelse(predicted_class == 0, "effusion", "normal"))

test_accuracy <- mean(stat_df_bestbatch_aug$class==stat_df_bestbatch_aug$predicted_class)
```

We compare the loss and accuracy on the training and validation sets before and after augmentation in Fig. \ref{fig:fig2}. We see that the validation loss and accuracy metrics stagnate from epoch 10 in both cases eventhough the validation accuracy shows some variance after augmentation. The accuracy level reached before augmentation was higher than the level reached after augmentation: about 71% against 67%.

```{r,eval=TRUE,fig.cap="\\label{fig:fig2} CNN loss and accuracy on training and validation set before and after augmentation",out.extra = "", fig.pos = 'h!', fig.align="center"}
p<-plot(history_bestbatch) + theme(legend.position = "bottom") + ggtitle("Before augmentation")
p2<-plot(history2) + theme(legend.position = "bottom") + ggtitle("After augmentation")
grid.arrange(p,p2,ncol=2)
```

We also compare the two models on the test sample. For the CNN after augmentation we obtain an accuracy of `r round(test_accuracy,2)` on the test set and the confusion matrix is:

```{r,eval=TRUE}
confusionMatrix(as.factor(stat_df_bestbatch_aug$predicted_class),as.factor(stat_df_bestbatch_aug$class))
```

The accuracy on the test set was slightly higher for the model before augmentation at 0.78. We note however that the two values are very close to each other. They each belong to the 95% confidence interval of the other. The Kappa measures are close to each other as well. Overall, the augmentation did not improve the model. 

```{r}
save.image("data_q1to6.RData")
```


# Question 8

***Implement a convolutional autoencoder (CAE) network.***

We use `image_data_generator`to normalize our data and `flow_images_from_directory` to split our images in batches of size 50 with mode `input`.

```{r,echo=FALSE}
b_size <- 50
train_datagen <- image_data_generator(rescale = 1/255)
train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  color_mode = "grayscale",
  target_size = c(64, 64),
  batch_size = b_size,
  class_mode = "input"
)

validation_datagen <- image_data_generator(rescale = 1/255)
validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  color_mode = "grayscale",
  target_size = c(64, 64),
  batch_size = b_size,
  class_mode = "input"
)
```

We define a convolutionnal autoencoder with 3 convolutionnal layers in the encoder and 4 convolutionnal layers in the decoder. The number of filters is a decresing function of the layer order in the encoder and we start from 64 filters. The pooling size is set at 2 in the first two layers and then 4 in the third one. The decoder structure is symetric to the encoder one, with an additional layer to obtain an image of the same dimension as the input. The total number of trainable parameters is 89,729.

```{r,echo=TRUE, eval = TRUE}
#### Convolutional Encoder 
filters_start <- 64
p_size <- 4

model_enc <- keras_model_sequential() 
model_enc %>%
  layer_conv_2d(filters = filters_start, kernel_size = c(2,2), padding ="same",
                activation = "relu",input_shape = c(64, 64, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2,2),padding ="same") %>%

  layer_conv_2d(filters = filters_start, kernel_size = c(2,2), padding ="same",
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2),padding ="same") %>% 
  
  layer_conv_2d(filters = filters_start/2, kernel_size = c(2,2), padding ="same",
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(p_size,p_size), padding ="same")
summary(model_enc)

#### Convolutional Decoder 

model_dec <- keras_model_sequential() 
model_dec %>%
  layer_conv_2d(filters = filters_start/2, kernel_size = c(3,3), 
                activation = "relu", padding = "same",
                input_shape = c(64/(2*2*p_size), 64/(2*2*p_size), filters_start/2))  %>%
  layer_upsampling_2d(size = c(2,2))  %>%
  
layer_conv_2d(filters = filters_start, kernel_size = c(3,3), 
                activation = "relu", padding = "same")  %>%
  layer_upsampling_2d(size = c(2,2))  %>%
  
  layer_conv_2d(filters = filters_start, kernel_size = c(3,3), 
                activation = "relu", padding = "same")  %>%
  layer_upsampling_2d(size = c(p_size,p_size))  %>%
  layer_conv_2d(filters = 1, kernel_size = c(1,1), 
                activation = "relu")  
summary(model_dec)

#### Autoencoder 
model_auto<-keras_model_sequential()
model_auto %>%model_enc%>%model_dec
```

We use the mean squared error as loss function and 5 epochs.

```{r,echo=TRUE}
# set seed for reproductibility
set.seed(42)
tensorflow::tf$random$set_seed(42)

model_auto %>% compile(
  loss = "mean_squared_error",
  #optimizer = optimizer_rmsprop(),
  optimizer = "adam",
  metrics = c("mean_squared_error")
)

# Fit the model
history_auto <- model_auto %>% fit_generator(
  train_generator,
  steps_per_epoch = 500/b_size,
  epochs = 5,
  validation_data = validation_generator,
  validation_steps = 100/b_size
)
model_auto %>% save_model_hdf5("auto_model.h5")
save.image("data_q1to8.RData")
```

We see that after 5 epochs, the training and validation metrics are similar and stagnate. More epochs would lead to overfitting.

```{r,eval=TRUE,fig.cap="\\label{fig:fig3} Autoencoder MSE on training and validation set",out.extra = "", fig.pos = 'h!', fig.align="center"}
plot(history_auto)
```


# Question 9

***Tune the more compact layer (z layer) with three configurations(width x heigth x filters) what you can free choose. To evaluate the z layer performance use this flattened layer as input in a random forest(or boosting) algorithm to classify the images.***

We use `tfruns` to run our convolutionnal autoencoder with different configurations of the encoder to obtain three different dimensions of the compact layer Z: (4,4,32), (4,4,64) and (8,8,64).

```{r,echo=TRUE}
training_run("cae_flags.R", flags = c(filters_nb_start = 64,
                                      pooling_size = 4))
training_run("cae_flags.R", flags = c(filters_nb_start = 128,
                                      pooling_size = 4))
training_run("cae_flags.R", flags = c(filters_nb_start = 128,
                                      pooling_size = 2))
tuning_res_dim <- tfruns::ls_runs() %>% filter(script == "cae_flags.R")
write_csv(tuning_res_dim,"tfruns_res_dim.csv")
```

On the training and validation set, the compact layer Z of dimension (8,8,64) gives the lowest MSE as shown in Table \ref{tab:tab2}.

```{r,eval=TRUE}
tfruns_res_dim <- read_csv("tfruns_res_dim.csv")
dimension <- c("(8,8,64)","(4,4,64)","(4,4,32)")
tfruns_res_dim %>% mutate(dimen = dimension) %>% select(c(27,8,9,2,3,4,5)) %>% kable(
  digits = 3,
  col.names = c("Dimension Z","Filter param","Pooling size", "Train. loss", "Val. loss","Train. MSE.","Val. MSE"),
  align = "c",
  caption = "\\label{tab:tab2} Z dimension tuning results - loss and MSE"
)
```

In the runs, we extracted and flattened the compact layer Z for each dimension for the training and the test sets and saved it in Rdata files. We fit an adaboost model with stumps on the training Z for each combination. For computation time and power, we ran the code in a Google Colab virtual machine with the attached notebook `Adaboost on encoder output.ipynb`.

```{r,eval=FALSE,echo = TRUE}
combina <-c("filter128_pool2","filter128_pool4","filter64_pool4")
for (com in combina){
  load(paste0("Conv_Encod_Flat_",com,".RData"))
  training <- data.frame(y=as.factor(y_radio_train),predict_enc_train)
  nzv <- nearZeroVar(training)
  training <- training[, -nzv]
  #test <- data.frame(y=as.factor(y_radio_test),predict_enc_test)
  training$y <- revalue(training$y, c("0"="effusion", "1"="normal"))
  #test$y <- revalue(test$y, c("0"="effusion", "1"="normal"))

  control <- trainControl(
    method = "repeatedcv",
    number = 5,
    repeats = 3,
    classProbs = TRUE
    allowParallel = TRUE,
    summaryFunction = twoClassSummary
  )

  grid <- expand.grid(
    .interaction.depth = 1,
    .n.trees = c(500, 1500, 3000),
    .shrinkage = .01,
    .n.minobsinnode = 10
  )

  metric <- "Accuracy"

  stump_adaboost <- train(y ~ .,
    data = training,
    method = "gbm",
    bag.fraction = 0.5,
    distribution = "adaboost",
    trControl = control,
    tuneGrid = grid,
    verbose = FALSE,
    metric = metric
  )

  save(stump_adaboost,file=paste0("stump_adaboost",com,".Rdata"))
}
```


```{r,eval=TRUE}
load("stump_adaboostfilter64_pool4.Rdata")
ada_64_4 <- stump_adaboost
load("stump_adaboostfilter128_pool2.Rdata")
ada_128_2 <- stump_adaboost
load("stump_adaboostfilter128_pool4.Rdata")
ada_128_4 <- stump_adaboost


resamps <- resamples(list(
  Dim4_4_32 = ada_64_4,
  Dim8_8_64 = ada_128_2,
  Dim4_4_64 = ada_128_4
))
```

We compare the accuracy obtained with the different dimensions in the training set in Fig.\ref{fig:fig4} where we plot the distributions of the ROC, Specificity and Sensitivity accross resamples. We see that in all the cases the ROC is poor, on average the dimensions (4,4,64) abd (8,8,64) have larger ROC tha (4,4,32) but the boxplots between the three are not well separated. The sensitivity and specificity are comparable in the 3 cases.

```{r,eval=TRUE,fig.cap="\\label{fig:fig4} Adaboost model accuracy with different dimensions of compact layer Z",out.extra = "", fig.pos = 'h!', fig.align="center"}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))
```

```{r,eval=TRUE}
load("Conv_Encod_Flat_filter64_pool4.RData")
test <- data.frame(y=as.factor(y_radio_test),predict_enc_test)
test$y <- revalue(test$y, c("0"="effusion", "1"="normal"))
pred_64_4 <- predict(ada_64_4, newdata = test)

df <- data.frame(obs = test$y, pred = pred_64_4, predict(ada_64_4, newdata = test, type = "prob"))
```

On the test set with dimension (4,4,32) we obtain the following confusion matrix and a ROC of `r round(twoClassSummary(df, lev = levels(df$obs))[1],3)`.
```{r,eval=TRUE}
confusionMatrix(data = pred_64_4, reference = test$y)
```

```{r,eval=TRUE}
load("Conv_Encod_Flat_filter128_pool4.RData")
test <- data.frame(y=as.factor(y_radio_test),predict_enc_test)
test$y <- revalue(test$y, c("0"="effusion", "1"="normal"))
pred_128_4 <- predict(ada_128_4, newdata = test)

df <- data.frame(obs = test$y, pred = pred_128_4, predict(ada_128_4, newdata = test, type = "prob"))
```

On the test set with dimension (4,4,64) we obtain the following confusion matrix and a ROC of `r round(twoClassSummary(df, lev = levels(df$obs))[1],3)`.

```{r, eval =TRUE}
confusionMatrix(data = pred_128_4, reference = test$y)
```

```{r,eval=TRUE}
load("Conv_Encod_Flat_filter128_pool2.RData")
test <- data.frame(y=as.factor(y_radio_test),predict_enc_test)
test$y <- revalue(test$y, c("0"="effusion", "1"="normal"))
pred_128_2 <- predict(ada_128_2, newdata = test)

df <- data.frame(obs = test$y, pred = pred_128_2, predict(ada_128_2, newdata = test, type = "prob"))
```

On the test set with dimension (8,8,64) we obtain the following confusion matrix and a ROC of `r round(twoClassSummary(df, lev = levels(df$obs))[1],3)`.

```{r,eval=TRUE}
confusionMatrix(data = pred_128_2, reference = test$y)
```

On the test set, all performance metrics are similar and accuracies 95% confidence intervals overlap. However, (8,8,64) has the largest accuracy and also the largest ROC. We choose (8,8,64) as our best dimension.


# Question 10

***Once the best performing z layer configuration has bee selected, perform a statistcal test to detect in which variables (nodes) there are significant differences between the two classes of images.***

We extract again the compact layer for the training set with dimension (8,8,64). For each variable, we run a t-test comparing the average value between the samples from the two classes and adjust for false discovery ratio with the Benjamini-Hochberg method.

```{r,echo=TRUE,eval=TRUE}
load("Conv_Encod_Flat_filter128_pool2.RData")
training <- data.frame(y=as.factor(y_radio_train),predict_enc_train)
nzv <- nearZeroVar(training)
training <- training[, -nzv]
training$y <- revalue(training$y, c("0"="effusion", "1"="normal"))


training.long <- training %>%
  pivot_longer(-y, names_to = "variables", values_to = "value")

stat.test <- training.long %>%
  group_by(variables) %>%
  t_test(value ~ y, comparisons = list(c("effusion","normal"))) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance()
write_csv(stat.test,"t_test_results.csv")
```

In Table \ref{tab:tab3}, we list the top 10 variables with the smallest p-values. In total we have `r nrow(stat.test %>% filter(p<0.05))` variables with significant difference in mean with a p-value cut off of 5% before adjustment. However, none of the variable has a significant difference in mean after adjustment for multi-testing.

```{r,echo=FALSE,eval=TRUE}
df<-stat.test %>% ungroup() %>% select(variables,statistic,p,p.adj) %>% arrange(p) %>% slice(1:10)
kable(df,
  digits = c(0,2,3,3),
  align = "c",
  caption = "\\label{tab:tab3} T-test on compact layer variables means between classes"
)
```

```{r,echo=FALSE}
write_csv(stat.test %>% filter(p<0.05)%>% arrange(p),"var_sign_dif_mean.csv")
```

# Question 11

***Visualize the results of the previous item using a Volcano plot.***

We compute the log fold change ($\log_2$) between the two classes and plot the variabes p-values in a log10 scale against the log fold change in Fig.\ref{fig:fig5}. The volcano plot shows a group of variables has significant p-values but among those very little have a large fold change: only three variables have a log fold change larger than 1 in absolute value. A lot of our significant unajusted p-values might actually highlight very small changes in means. This volcano plot tells us that overall our variables have little difference betweenn the two classes, which is in line with the fact thay we find they have little classification power.

```{r,echo=TRUE,eval=TRUE}
stat.test <- stat.test %>% ungroup() %>% arrange(variables)

lfc<- training.long %>%
  dplyr::group_by(variables,y) %>% 
  dplyr::summarise(log_mean = log2(mean(value)))

lfc <- pivot_wider(lfc, id_cols=variables, names_from = y, values_from = log_mean) %>% 
  mutate(lfc=effusion-normal) %>% 
  arrange(variables)

stat.test$lfc <- lfc$lfc
stat.test$log10pval <- -log10(stat.test$p)
```

```{r,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig5} Volcano plot of differences in means of Z varibales",out.extra = "", fig.pos = 'h!', fig.align="center"}
ggplot(stat.test)+geom_point(aes(x=lfc,y=log10pval)) + 
  xlab("log fold change") + ylab("-log10 p-value") +
  geom_hline(yintercept = -log10(0.05), col = "red", lty = 2)
```

# Question 12

CNNs are mostly used for processing images. The input can generally be 3 channels (for color images) or 1 channel (for grayscale images). The 1 or 3 channels form the volume of the input image (using 2D matrixes for each channel) and is reflected in the layers of the network. The network uses filters in its layers to find patterns and one can use different lensâ€™ sizes (3x3, 5x5, etc, generally odd numbers). Another important element of this type networks if the pooling layer, which is a sort of non-linear down-sampling layer. The network allows for network reconfiguration in each training epoch in order to improve accuracy and contribute to gradient calculation speed. On the output side of the CNN, after all convolutions are concluded, output is generated via a fully connected dense layer.

CAEs consist of two CNNs connected back to back, where one is referred to as the encoder (input network) and the other decoder (output network). The task of the encoder is to produce a data representation of the input data in a lower dimensional space (occupying less bytes) using a non-linear transformation. The task of the decoder is to re-interpret the encoded information and produce a result that attempts to resemble the original / input information, with as less possible loss of data as possible. Generally this is used also for images. The benefit of having two networks work separately is that, eventually, when all the information has been encoded, the two networks can be disconnected and use a different approach in place of the decoder.

In this work, the CNN classification has proved to be much more accurate than the classification based on the compact layer output by the encoder. Such results must be considered carefully given the limitations of our models and tuning process. Our CNN has much more parameters than our CAE which was limited by design. We also experienced a lot of variance in performance metrics on validation and test sets which complicated the tuning of hyperparameters. With more samples, more time in our hands and more computing power to perform a extensive grid search with good estimates of performance metrics, but also more experience, the results would have been certainly different. A lesson learned here is that given the difficulties in training a good netwrok, relying on pre-trained neural networks as base structure is a very reasonable option when training ressources (time, computing power, samples) are scarce. 

